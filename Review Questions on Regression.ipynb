{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Questions-I from MDSC-301(P)\n",
    "\n",
    "----------------------------------------------------------------\n",
    "Author  :  **Balu Niranjan**\n",
    "\n",
    "Roll no : **21229**\n",
    "\n",
    "Date    :  **06- September- 2022**\n",
    "\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.Which Linear Regression training algorithm can you use if you have\n",
    "a training set with millions of features?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "    - Gradient Descent (Stochastic, Mini Batch, Batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. Suppose the features in your training set have very different scales.\n",
    "Which algorithms might suffer from this, and how? What can you\n",
    "do about it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "    - Gradient Descent will suffer.\n",
    "    - It will take longer time to converge as the shape of the cost function will become an elongated bowl shape.\n",
    "    - We should perform feature scaling on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. Suppose you use Batch Gradient Descent and you plot the validation\n",
    "error at every epoch. If you notice that the validation error\n",
    "consistently goes up, what is likely going on? How can you fix this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "    - If the validation error goes up after every epoch, then the learning rate of the Batch Gradient Descent is very high and the model is diverging.\n",
    "    - We can reduce the learning rate and and train the model again.\n",
    "    - If training error is not going up, then the model is overfitting the training set.\n",
    "    - We need to stop training or train with regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. Is it a good idea to stop Mini-batch Gradient Descent immediately\n",
    "when the validation error goes up?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "    - Stochastic GD and Mini-batch GD is not guaranteed to make progress at every single training iteration.\n",
    "    - So if we immediately stop training when the validation error goes up,we may stop before the optimum is reached.\n",
    "    - It is better to save the model at regular intervals and when there is no improvement for a long time, we can go back to the previously saved model with better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. Suppose you are using Polynomial Regression. You plot the learning\n",
    "curves and you notice that there is a large gap between the training\n",
    "error and the validation error. What is happening? What are three\n",
    "ways to solve this?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "    If the validation error is more than the training error, then the model is overfitting.\n",
    "\n",
    "    Solution:\n",
    "        1. Reduce the polynomial degree.\n",
    "        2. Regularization\n",
    "        3. Increase the size of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6. Suppose you are using Ridge Regression and you notice that the\n",
    "training error and the validation error are almost equal and fairly\n",
    "high. Would you say that the model suffers from high bias or high\n",
    "variance? Should you increase the regularization hyperparameter $\\alpha$\n",
    "or reduce it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "    - If both training and validation error are almost equal and fairly high, then the model is underfitting the training set.\n",
    "    - It has high bias\n",
    "    - We should reduce the hyperparamater(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7. Why would you want to use:   \n",
    "    (1) Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?   \n",
    "        (2) Lasso instead of Ridge Regression?   \n",
    "      (3) Elastic Net instead of Lasso?**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?\n",
    "**ANS:** \n",
    "    \n",
    "    - Ridge Regression is preferred over Linear Regression as they are optmized for predictions rather than inference.\n",
    "    - Ridge regression regularizes the data, which helps them in working better with unseen data and also allows you to use complex models and avoid over-fitting at the same time.\n",
    "\n",
    "##### Lasso instead of Ridge Regression?\n",
    "**ANS:** \n",
    "\n",
    "    - Lasso Regression can be used instead of Ridge Regression in cases where only a small number of predictor variables are actually significant.\n",
    "    - Lasso is able to shrink the insignificant variables to zero and remove them from the model.\n",
    "\n",
    "##### Elastic Net instead of Lasso?\n",
    "**ANS:** \n",
    "\n",
    "    - Elastic net is a hybrid of ridge and LASSO .Similar to LASSO, elastic net can generate reduced models by generating zero-valued coefficients. \n",
    "    - It is said that elastic net technique can outperform lasso on data with highly correlated predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8. Can you name four of the main challenges in Machine Learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "    1. Poor Quality of the data\n",
    "    2. Overfitting and Underfitting the data\n",
    "    3. Irrelevant Features\n",
    "    4. Non-Representative Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9. If your model performs great on the training data but generalizes\n",
    "poorly to new instances, what is happening? Can you name three\n",
    "possible solutions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "    If the model performs great on the training data but generalizes poorly on new instances, then the model is clearly overfitting the training set.\n",
    "    Solutions:\n",
    "        1. Cross Validation\n",
    "        2. Train with more data\n",
    "        3. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10. What is a test set, and why would you want to use it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "    Test set is the subset of the dataset that we use to test our model with, after the model has gone through initial checking by the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.  What is the purpose of a validation set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "    Validation set is the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12. What are different loss functions? Exaplain their importance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "    The diffrent loss functions are:\n",
    "        1. Mean Square Error/Quadratic Loss\n",
    "            - Used to ensure that the trained model has no outlier predictions with huge errors\n",
    "            - If the model makes a single very bad prediction, the squaring part of the function magnifies the error.\n",
    "            \n",
    "        2. Mean Absolute Error\n",
    "            - All the errors will be weighted on the same linear scale unlike MSE.\n",
    "            - The large errors coming from the outliers end up being weighted the exact same as lower errors.\n",
    "            \n",
    "        3. Hinge Loss/Multi class SVM Loss\n",
    "            - Hinge loss leads to better accuracy and some sparsity at the cost of much less sensitivity regarding probabilities.\n",
    "            \n",
    "        4. Cross Entropy Loss/Negative Log Likelihood\n",
    "            - Used in Classification problems.\n",
    "            - Distributions with long tails can be modeled poorly with too much weight given to the unlikely events.\n",
    "\n",
    "    - A loss function is a measure of how good our prediction model does in terms of being able to predict the expected outcome.\n",
    "    - We convert the learning problem into an optimization problem, define a loss function and then optimize the algorithm to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13. Explain the following:   \n",
    "    (1) Gradient descent   \n",
    "    (2) Mini-batch gradient descent   \n",
    "    (3) Batch gradient   \n",
    "    (4) Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "##### Gradient Descent: \n",
    "    - Gradient descent is an optimization algorithm which is commonly-used to train machine learning models.\n",
    "    - It is used for minimizing the cost function and for updating the parameters of the learning model. \n",
    "\n",
    "##### Batch Gradient Descent:\n",
    "    - It processes all the training samples for each iteration of gradient descent.\n",
    "    - Computationally expensive if the number of training samples are large.\n",
    "    \n",
    "##### Mini Batch Gradient Descent:\n",
    "    - It is similar to Batch Gradient Descent.\n",
    "    - Here we use a batch of a fixed number of training examples which is less than the actual dataset and call it a mini-batch.\n",
    "    - It works faster than both stochastic and Batch Gradient Descent.\n",
    "    \n",
    "\n",
    "##### Stochastic Gradient Descent:\n",
    "    - It processes 1 training sample per iteration.\n",
    "    - The parameters are being updated even after one iteration in which only a single example has been processed.\n",
    "    - It is very fast when compared to Batch Gradient Descent.\n",
    "    - Even when the number of training sample is large, it will only process one at a time, which can cause an overhead as the iterations are very large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14. What is learning rate?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "    - Learning rate (step size)\n",
    "    - It is the size of the steps that are taken to reach the minimum.\n",
    "    - It is typically a small value, and it is evaluated and updated based on the behavior of the cost function.\n",
    "    - High learning rates result in larger steps but risks overshooting the minimum.\n",
    "    - A low learning rate has small step sizes, it has the advantage of more precision.\n",
    "    - But the number of iterations compromises overall efficiency as this takes more time and computations to reach the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15. Define the following terms. Explain their importance in the data analysis.   \n",
    "    - $R^2$   \n",
    "    - Adjusted $R^2$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "$R^2:$ \n",
    "    \n",
    "    - R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n",
    "    - R-squared explains to what extent the variance of one variable explains the variance of the second variable. \n",
    "    - Even if we are adding redundant variables to the data, the value of R-squared does not decrease.\n",
    "    - It either remains the same or increases with the addition of new independent variables.\n",
    "    \n",
    "\n",
    "  $R^{2}$ = $1$ $-$ $\\frac{RSS}{TSS}$, where \n",
    "  \n",
    "  $RSS$ = $\\sum$ $(y - \\hat{y})^{2}$  ,  $TSS$ = $\\sum$ $(y - \\bar{y})^{2}$\n",
    "  \n",
    "  $y$ = Actual Value    \n",
    "  $\\hat{y}$ = Observed Value    \n",
    "  $\\bar{y}$ = Mean Value\n",
    "  \n",
    "\n",
    "Adjusted $R^2:$\n",
    "    \n",
    "    - Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
    "    - The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected.\n",
    "    - Adjusted R-squared is used to determine how reliable the correlation is and how much it is determined by the addition of independent variables.\n",
    "    \n",
    "    \n",
    " $Adjusted $ $R^{2}$ = $\\left \\{1- [\\frac{(1-R^{2})(n-1)}{(n-k-1)}]\\right \\}$ , where\n",
    " \n",
    "  $n$ =  number of data points in our dataset\n",
    "  \n",
    "  $k$ =  number of independent variables\n",
    "  \n",
    "  $R$ =  R-squared values determined by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q16. Explain One-Hot Encoding and Label Encoding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "Label Encoding:\n",
    "\n",
    "    - Label Encoding is a popular encoding technique for handling categorical variables.\n",
    "    - In this technique, each label is assigned a unique integer based on alphabetical ordering.\n",
    "    - It is also called Integer Encoding.\n",
    "\n",
    "\n",
    "One-Hot Encoding:\n",
    "\n",
    "    - One-Hot Encoding is a popular technique for treating categorical variables.\n",
    "    - It creates additional features based on the number of unique values in the categorical feature.\n",
    "    - Every unique value in the category will be added as a feature.\n",
    "    - One-Hot Encoding can be said as the process of creating dummy variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q17. What are the assumption on Naive Bayes algorithm in classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "    Navie Bayes have the following assumptions:\n",
    "        1. The prior probabilities for each class is either uniform or empirical\n",
    "        2. The likelihood probabilities are Gaussian\n",
    "        3. All the features (or predictors) are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q18. What is the difference between classification and regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "Regression:\n",
    "\n",
    "    - A regression algorithm can predict a discrete value which is in the form of an integer quantity.\n",
    "    - A regression algorithm can be used in this case to predict the height of any student based on their weight, gender, diet, or subject major.\n",
    "Classification:\n",
    "\n",
    "    - A classification algorithm can predict a continuous value if it is in the form of a class label probability.\n",
    "    - Classification can be used to analyse whether an email is a spam or not spam.\n",
    "    - The algorithm checks the keywords in an email and the sender’s address is to find out the probability of the email being spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q19. How to ensure that the model is not overfitting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "    1. Cross-Validating the data\n",
    "    2. Augmenting the data\n",
    "    3. Proper Feature Selection\n",
    "    4. Regularize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q20. List the main advantage of Naive Bayes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "    The following are some of the advantages of the Naive Bayes: \n",
    "\n",
    "        1. It is very quick and can save a lot of time.\n",
    "        2. Suitable for solving multi-class prediction problems\n",
    "        3. If the independence of features holds true, then Navie Bayes can perform better than other models.\n",
    "        4. It requires much less training data.\n",
    "        5. It's better suited for categorical predictors than numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q21. What you shoud do when your model is suffering from:   \n",
    "    - Low bias and high variance?   \n",
    "    - High bias and low variance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "Low Bias and High Variance:\n",
    "\n",
    "       - Models that overfit have low bias and high variance.\n",
    "       - We can reduce the complexity of the model to get a good bias-variance trade off\n",
    "       - We can remove irrelevant features from the model.\n",
    "       - We can use Regularization techniques to reduce overfitting.\n",
    "       - We can add more training data to prevent overfitting.\n",
    "       - Decreasing the degree of the polynomial  of the hypothesis function is another approach to solve the problem.\n",
    "       \n",
    "High Bias and Low Variance:\n",
    "\n",
    "       - Models that underfit have high bias and low variance.\n",
    "       - It is usually caused when the hypothesis function is too simple or has very less features.\n",
    "       - We can increase the number of features to overcome this.\n",
    "       - We can also do feature engineering.\n",
    "       - Increasing the degree of the polynomial  of the hypothesis function is another approach to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q22. What is the 'Naive' in the Naive Bayes Classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "    - Naive Bayes Classifier is 'naive' because it makes the assumption that features of a measurement are independent of each other.\n",
    "    - This is 'naive' because it is almost never true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q23. What is bias-variance tradeoff in Machine Learning ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "    \n",
    "    - The goal of any supervised machine learning algorithm is to achieve low bias and low variance.\n",
    "    - The algorithm should achieve good prediction performance.\n",
    "    - The parameterization of machine learning algorithms is often a battle to balance out bias and variance.\n",
    "    - Increasing the bias will decrease the variance and increasing the variance will decrease the bias.\n",
    "    - The algorithms we choose and the way we choose to configure them are finding different balances in this trade-off for the problem.\n",
    "    - Bias and variance provide the tools to understand the behavior of machine learning algorithms in the pursuit of predictive performance.\n",
    "    - Dimensionality reduction, feature selection can decrease variance by simplifying models.\n",
    "    - A larger training set tends to decrease variance.\n",
    "    - Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q24. Explain different trade-offs in Machine Learning algorithms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "       The different trade-offs in Machine Learning are:\n",
    "           1. Bias- Variance Trade-off\n",
    "           2. Precision- Recall Trade-off\n",
    "           3. Fairness - Privacy Trade-off\n",
    "           4. Accuracy - Interpretability Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q25. What is cross-validation and how it is useful in training ML models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "    Cross-validation is a technique in which we train our model using the subset of the data-set and then evaluate using the complementary subset of the data-set.\n",
    "    It is used for avoiding overfitting of the data.\n",
    "    The different approaches for cross-validation are:\n",
    "    \n",
    "        1. Hold Out Method :        - Involves removing a part of the training data and using it to get predictions from the model trained on rest of the data.\n",
    "                                    - The error estimation then tells how our model is doing on unseen data or the validation set.\n",
    "                                    - It suffers from issues of high variance.\n",
    "                             \n",
    "        2. k-fold Cross Validation: - In K Fold cross validation, the data is divided into k subsets.\n",
    "                                    - The holdout method is repeated k times(one of the k subsets is used as the test set/ validation set)\n",
    "                                    - The other k-1 subsets are put together to form a training set. (k= 5/10 preffered)\n",
    "                                    - It reduces bias as we are using most of the data for fitting.\n",
    "                                    - It reduces variance as most of the data is also being used in validation set.\n",
    "                                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
